{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras/tenserflow packages/classes\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# import sklearn packages\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import nltk packages/classes\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# import utility libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set display preferences\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['marketplace',\n",
    "        'customer_id',\n",
    "        'review_id',\n",
    "        'product_id',\n",
    "        'product_parent',\n",
    "        'product_title',\n",
    "        'product_category',\n",
    "        'star_rating',\n",
    "        'helpful_votes',\n",
    "        'total_votes',\n",
    "        'vine',\n",
    "        'verified_purchase',\n",
    "        'review_headline',\n",
    "        'review_body',\n",
    "        'review_date']\n",
    "\n",
    "df = pd.read_csv('amazon_reviews_us_Luggage_v1_00.tsv',\n",
    "                 sep='\\t',\n",
    "                 usecols = cols)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# drop reviews with < 10 votes or vine\n",
    "df = df.loc[(df.helpful_votes > 10) & (df.vine == 'N')]\n",
    "\n",
    "# drop duplicate reviews\n",
    "df.drop_duplicates(subset=['review_body'], inplace=True)\n",
    "\n",
    "# reset index\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Target Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"uncomment to generate target dataframes\"\"\"\n",
    "# # helpfulness ratio\n",
    "# df['helpful_score'] = df.helpful_votes / df.total_votes\n",
    "\n",
    "# # helpfulness > global median\n",
    "# df['helpful_1'] = np.where(df.helpful_score > df.helpful_score.median(), 1, 0)\n",
    "\n",
    "# # product median helpful votes\n",
    "# df['median_helpful_votes'] = df.product_id.apply(lambda x: df.groupby('product_id').helpful_votes.median()[x])\n",
    "\n",
    "# # helpful votes > product median helpful votes\n",
    "# df['helpful_2'] = np.where(df.helpful_votes > df.median_helpful_votes, 1, 0)\n",
    "\n",
    "# # unhelpful votes\n",
    "# df['unhelpful_votes'] = df.total_votes - df.helpful_votes\n",
    "\n",
    "# # product median unhelpful votes\n",
    "# df['median_unhelpful_votes'] = df.product_id.apply(lambda x: df.groupby('product_id').unhelpful_votes.median()[x])\n",
    "\n",
    "# # unhelpful votes > review median helpful votes\n",
    "# df['unhelpful'] = np.where(df.unhelpful_votes > df.median_unhelpful_votes, 1, 0)\n",
    "\n",
    "# df_helpful_1 = df[['helpful_1', 'review_body']]\n",
    "# df_helpful_2 = df[['helpful_2', 'review_body']]\n",
    "# df_unhelpful = df[['unhelpful', 'review_body']]\n",
    "\n",
    "# df_helpful_1.columns = ['helpful', 'text']\n",
    "# df_helpful_2.columns = ['helpful', 'text']\n",
    "# df_unhelpful.columns = ['unhelpful', 'text']\n",
    "\n",
    "# df_helpful_1.to_csv('helpful_1.csv')\n",
    "# df_helpful_2.to_csv('helpful_2.csv')\n",
    "# df_unhelpful.to_csv('unhelpful.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_helpful_1 = pd.read_csv('helpful_1.csv')\n",
    "df_helpful_2 = pd.read_csv('helpful_2.csv')\n",
    "df_unhelpful = pd.read_csv('unhelpful.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "df['tokens'] = df.review_body.apply(word_tokenize)\n",
    "df['stopped_tokens'] = df.tokens.apply(lambda x: [word.lower() for word in x if word not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk import FreqDist\n",
    "df['freqdist'] = df.stopped_tokens.apply(FreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['most_common'] = df.freqdist.apply(lambda x: x.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "df['lemmas'] = df.stopped_tokens.apply(lambda x: [WordNetLemmatizer().lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oversized'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize('oversized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stopped_tokens</th>\n",
       "      <th>freqdist</th>\n",
       "      <th>most_common</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>20761040</td>\n",
       "      <td>R11IBSD5E6HPSD</td>\n",
       "      <td>B002B3FWXY</td>\n",
       "      <td>677901073</td>\n",
       "      <td>Travelon Anti-Theft Classic Messenger Bag</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>This bag was on my shoulder and it just fell t...</td>\n",
       "      <td>The strap broke!!!  It was supposed to be anti...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>[The, strap, broke, !, !, !, It, was, supposed...</td>\n",
       "      <td>[the, strap, broke, it, supposed, anti-theft, ...</td>\n",
       "      <td>{'the': 1, 'strap': 2, 'broke': 2, 'it': 2, 's...</td>\n",
       "      <td>[(i, 5), (n't, 3), (strap, 2), (broke, 2), (it...</td>\n",
       "      <td>[the, strap, broke, it, supposed, anti-theft, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>23857312</td>\n",
       "      <td>R3NPROA23JJRFF</td>\n",
       "      <td>B00V6FKB5M</td>\n",
       "      <td>909535974</td>\n",
       "      <td>MOIERG Vintage Trolley Luggage 2tone TSA</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>This product is absolutely BEAUTIFUL. I ordere...</td>\n",
       "      <td>This product is absolutely BEAUTIFUL.  I order...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>[This, product, is, absolutely, BEAUTIFUL, ., ...</td>\n",
       "      <td>[this, product, absolutely, beautiful, i, orde...</td>\n",
       "      <td>{'this': 1, 'product': 1, 'absolutely': 1, 'be...</td>\n",
       "      <td>[(i, 5), (august, 2), (this, 1), (product, 1),...</td>\n",
       "      <td>[this, product, absolutely, beautiful, i, orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>12318409</td>\n",
       "      <td>R2KVWAYBPWK1OV</td>\n",
       "      <td>B011KEPZG8</td>\n",
       "      <td>919734058</td>\n",
       "      <td>Iblue Canvas Leather Weekend Shoulder Duffels ...</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>My boyfriend wouldn't be without this for travel!</td>\n",
       "      <td>This review is for the Iblue Oversized Leather...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>[This, review, is, for, the, Iblue, Oversized,...</td>\n",
       "      <td>[this, review, iblue, oversized, leather, canv...</td>\n",
       "      <td>{'this': 4, 'review': 2, 'iblue': 2, 'oversize...</td>\n",
       "      <td>[(br, 12), (bag, 7), (leather, 5), (the, 5), (...</td>\n",
       "      <td>[this, review, iblue, oversized, leather, canv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>25513669</td>\n",
       "      <td>R1FLOE9E4ODIGR</td>\n",
       "      <td>B00VBDT55G</td>\n",
       "      <td>995661027</td>\n",
       "      <td>Hynes Eagle Travel Backpack 40L Flight Approve...</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>It's perfect!</td>\n",
       "      <td>I'm just packing for my trip to Europe, and th...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>[I, 'm, just, packing, for, my, trip, to, Euro...</td>\n",
       "      <td>[i, 'm, packing, trip, europe, luggage, i, 'm,...</td>\n",
       "      <td>{'i': 6, ''m': 2, 'packing': 1, 'trip': 1, 'eu...</td>\n",
       "      <td>[(i, 6), ('m, 2), (bag, 2), (packing, 1), (tri...</td>\n",
       "      <td>[i, 'm, packing, trip, europe, luggage, i, 'm,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>11235780</td>\n",
       "      <td>R6XTEZCSCUJ4J</td>\n",
       "      <td>B00SXKUEIC</td>\n",
       "      <td>43152132</td>\n",
       "      <td>AmazonBasics 4 Piece Small Packing Cube Set</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I'm very pleased; they seem to be well made wi...</td>\n",
       "      <td>My husband and I are travelling to NYC in the ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>[My, husband, and, I, are, travelling, to, NYC...</td>\n",
       "      <td>[my, husband, i, travelling, nyc, near, future...</td>\n",
       "      <td>{'my': 1, 'husband': 1, 'i': 7, 'travelling': ...</td>\n",
       "      <td>[(i, 7), (well, 3), (future, 2), ('m, 2), (br,...</td>\n",
       "      <td>[my, husband, i, travelling, nyc, near, future...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     20761040  R11IBSD5E6HPSD  B002B3FWXY       677901073   \n",
       "1          US     23857312  R3NPROA23JJRFF  B00V6FKB5M       909535974   \n",
       "2          US     12318409  R2KVWAYBPWK1OV  B011KEPZG8       919734058   \n",
       "3          US     25513669  R1FLOE9E4ODIGR  B00VBDT55G       995661027   \n",
       "4          US     11235780   R6XTEZCSCUJ4J  B00SXKUEIC        43152132   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0          Travelon Anti-Theft Classic Messenger Bag          Luggage   \n",
       "1           MOIERG Vintage Trolley Luggage 2tone TSA          Luggage   \n",
       "2  Iblue Canvas Leather Weekend Shoulder Duffels ...          Luggage   \n",
       "3  Hynes Eagle Travel Backpack 40L Flight Approve...          Luggage   \n",
       "4        AmazonBasics 4 Piece Small Packing Cube Set          Luggage   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          1.0           29.0         31.0    N                 Y   \n",
       "1          5.0           11.0         15.0    N                 Y   \n",
       "2          5.0           20.0         22.0    N                 N   \n",
       "3          5.0           34.0         38.0    N                 Y   \n",
       "4          5.0           20.0         23.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0  This bag was on my shoulder and it just fell t...   \n",
       "1  This product is absolutely BEAUTIFUL. I ordere...   \n",
       "2  My boyfriend wouldn't be without this for travel!   \n",
       "3                                      It's perfect!   \n",
       "4  I'm very pleased; they seem to be well made wi...   \n",
       "\n",
       "                                         review_body review_date  \\\n",
       "0  The strap broke!!!  It was supposed to be anti...  2015-08-31   \n",
       "1  This product is absolutely BEAUTIFUL.  I order...  2015-08-31   \n",
       "2  This review is for the Iblue Oversized Leather...  2015-08-31   \n",
       "3  I'm just packing for my trip to Europe, and th...  2015-08-31   \n",
       "4  My husband and I are travelling to NYC in the ...  2015-08-31   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [The, strap, broke, !, !, !, It, was, supposed...   \n",
       "1  [This, product, is, absolutely, BEAUTIFUL, ., ...   \n",
       "2  [This, review, is, for, the, Iblue, Oversized,...   \n",
       "3  [I, 'm, just, packing, for, my, trip, to, Euro...   \n",
       "4  [My, husband, and, I, are, travelling, to, NYC...   \n",
       "\n",
       "                                      stopped_tokens  \\\n",
       "0  [the, strap, broke, it, supposed, anti-theft, ...   \n",
       "1  [this, product, absolutely, beautiful, i, orde...   \n",
       "2  [this, review, iblue, oversized, leather, canv...   \n",
       "3  [i, 'm, packing, trip, europe, luggage, i, 'm,...   \n",
       "4  [my, husband, i, travelling, nyc, near, future...   \n",
       "\n",
       "                                            freqdist  \\\n",
       "0  {'the': 1, 'strap': 2, 'broke': 2, 'it': 2, 's...   \n",
       "1  {'this': 1, 'product': 1, 'absolutely': 1, 'be...   \n",
       "2  {'this': 4, 'review': 2, 'iblue': 2, 'oversize...   \n",
       "3  {'i': 6, ''m': 2, 'packing': 1, 'trip': 1, 'eu...   \n",
       "4  {'my': 1, 'husband': 1, 'i': 7, 'travelling': ...   \n",
       "\n",
       "                                         most_common  \\\n",
       "0  [(i, 5), (n't, 3), (strap, 2), (broke, 2), (it...   \n",
       "1  [(i, 5), (august, 2), (this, 1), (product, 1),...   \n",
       "2  [(br, 12), (bag, 7), (leather, 5), (the, 5), (...   \n",
       "3  [(i, 6), ('m, 2), (bag, 2), (packing, 1), (tri...   \n",
       "4  [(i, 7), (well, 3), (future, 2), ('m, 2), (br,...   \n",
       "\n",
       "                                              lemmas  \n",
       "0  [the, strap, broke, it, supposed, anti-theft, ...  \n",
       "1  [this, product, absolutely, beautiful, i, orde...  \n",
       "2  [this, review, iblue, oversized, leather, canv...  \n",
       "3  [i, 'm, packing, trip, europe, luggage, i, 'm,...  \n",
       "4  [my, husband, i, travelling, nyc, near, future...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>...</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_length</th>\n",
       "      <th>num_exclamation</th>\n",
       "      <th>num_question</th>\n",
       "      <th>median_star_rating</th>\n",
       "      <th>relative_star_rating</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>20761040</td>\n",
       "      <td>R11IBSD5E6HPSD</td>\n",
       "      <td>B002B3FWXY</td>\n",
       "      <td>677901073</td>\n",
       "      <td>Travelon Anti-Theft Classic Messenger Bag</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>This bag was on my shoulder and it just fell t...</td>\n",
       "      <td>The strap broke!!!  It was supposed to be anti...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>318</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>23857312</td>\n",
       "      <td>R3NPROA23JJRFF</td>\n",
       "      <td>B00V6FKB5M</td>\n",
       "      <td>909535974</td>\n",
       "      <td>MOIERG Vintage Trolley Luggage 2tone TSA</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>This product is absolutely BEAUTIFUL. I ordere...</td>\n",
       "      <td>This product is absolutely BEAUTIFUL.  I order...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>437</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>12318409</td>\n",
       "      <td>R2KVWAYBPWK1OV</td>\n",
       "      <td>B011KEPZG8</td>\n",
       "      <td>919734058</td>\n",
       "      <td>Iblue Canvas Leather Weekend Shoulder Duffels ...</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>My boyfriend wouldn't be without this for travel!</td>\n",
       "      <td>This review is for the Iblue Oversized Leather...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1951</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>25513669</td>\n",
       "      <td>R1FLOE9E4ODIGR</td>\n",
       "      <td>B00VBDT55G</td>\n",
       "      <td>995661027</td>\n",
       "      <td>Hynes Eagle Travel Backpack 40L Flight Approve...</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>It's perfect!</td>\n",
       "      <td>I'm just packing for my trip to Europe, and th...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>415</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>11235780</td>\n",
       "      <td>R6XTEZCSCUJ4J</td>\n",
       "      <td>B00SXKUEIC</td>\n",
       "      <td>43152132</td>\n",
       "      <td>AmazonBasics 4 Piece Small Packing Cube Set</td>\n",
       "      <td>Luggage</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>I'm very pleased; they seem to be well made wi...</td>\n",
       "      <td>My husband and I are travelling to NYC in the ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     20761040  R11IBSD5E6HPSD  B002B3FWXY       677901073   \n",
       "1          US     23857312  R3NPROA23JJRFF  B00V6FKB5M       909535974   \n",
       "2          US     12318409  R2KVWAYBPWK1OV  B011KEPZG8       919734058   \n",
       "3          US     25513669  R1FLOE9E4ODIGR  B00VBDT55G       995661027   \n",
       "4          US     11235780   R6XTEZCSCUJ4J  B00SXKUEIC        43152132   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0          Travelon Anti-Theft Classic Messenger Bag          Luggage   \n",
       "1           MOIERG Vintage Trolley Luggage 2tone TSA          Luggage   \n",
       "2  Iblue Canvas Leather Weekend Shoulder Duffels ...          Luggage   \n",
       "3  Hynes Eagle Travel Backpack 40L Flight Approve...          Luggage   \n",
       "4        AmazonBasics 4 Piece Small Packing Cube Set          Luggage   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes  ... verified_purchase  \\\n",
       "0          1.0           29.0         31.0  ...                 Y   \n",
       "1          5.0           11.0         15.0  ...                 Y   \n",
       "2          5.0           20.0         22.0  ...                 N   \n",
       "3          5.0           34.0         38.0  ...                 Y   \n",
       "4          5.0           20.0         23.0  ...                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0  This bag was on my shoulder and it just fell t...   \n",
       "1  This product is absolutely BEAUTIFUL. I ordere...   \n",
       "2  My boyfriend wouldn't be without this for travel!   \n",
       "3                                      It's perfect!   \n",
       "4  I'm very pleased; they seem to be well made wi...   \n",
       "\n",
       "                                         review_body review_date  \\\n",
       "0  The strap broke!!!  It was supposed to be anti...  2015-08-31   \n",
       "1  This product is absolutely BEAUTIFUL.  I order...  2015-08-31   \n",
       "2  This review is for the Iblue Oversized Leather...  2015-08-31   \n",
       "3  I'm just packing for my trip to Europe, and th...  2015-08-31   \n",
       "4  My husband and I are travelling to NYC in the ...  2015-08-31   \n",
       "\n",
       "  review_length  num_exclamation  num_question  median_star_rating  \\\n",
       "0           318                3             3                 4.0   \n",
       "1           437                2             2                 5.0   \n",
       "2          1951                9             9                 5.0   \n",
       "3           415                1             1                 5.0   \n",
       "4           707                1             1                 5.0   \n",
       "\n",
       "   relative_star_rating  popularity  \n",
       "0                  0.25          15  \n",
       "1                  1.00           1  \n",
       "2                  1.00           1  \n",
       "3                  1.00           3  \n",
       "4                  1.00           1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structural features\n",
    "df['review_length'] = df.review_body.apply(lambda x: len(x))\n",
    "df['num_exclamation_pts'] = df.review_body.str.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta-data features\n",
    "df['prod_median_star_rating'] = df.product_id.apply(lambda x: df.groupby('product_id').star_rating.median()[x])\n",
    "df['review_relative_star_rating'] = df.star_rating / df.prod_median_star_rating\n",
    "df['num_reviews'] = df.product_id.apply(lambda x: df.groupby('product_id').size()[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train-val-test split\n",
    "y = df_unhelpful['unhelpful']\n",
    "X = df_unhelpful['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train LSTM with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create sequences of tokens of uniform length for all reviews (~ 1 min.)\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_tok_pad = sequence.pad_sequences(X_train_tok, maxlen=100)\n",
    "\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tok_pad = sequence.pad_sequences(X_val_tok, maxlen=100)\n",
    "\n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_tok_pad = sequence.pad_sequences(X_test_tok, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build and train neural network (~ 80 sec./epoch)\n",
    "checkpoint = callbacks.ModelCheckpoint('lstm_model_1.h5', \n",
    "                                       save_best_only=True)\n",
    "\n",
    "embedding_size = 128\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(20000, embedding_size, \n",
    "                                    input_shape=(100,)))\n",
    "model.add(tf.keras.layers.LSTM(25, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(25, return_sequences=True))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_tok_pad, y_train, epochs=5, batch_size=1024, \n",
    "                    validation_data=(X_val_tok_pad, y_val),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define a function to visualize ROC curve and AUC\n",
    "def roc_it(y_true, y_pred, model_name, figsize=(12,10)):\n",
    "    '''Plot ROC curve with AUC value overlaid on plot.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y_true: ground-truth labels\n",
    "    y_pred: predicted labels\n",
    "    model_name: name to print in the plot title\n",
    "    \n",
    "    Dependencies:\n",
    "    pandas aliased as pd\n",
    "    sklearn.metrics.roc_curve\n",
    "    sklearn.metrics.auc\n",
    "    matplotlib.pyplot aliased as plt\n",
    "    '''\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    auc_value = round(auc(fpr, tpr), 2)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0.0,1.0], [0.0,1.0], linestyle='--')\n",
    "    plt.text(x=0.0, y=0.95, s='AUC: {}'.format(auc_value))\n",
    "    plt.title('ROC curve for {}'.format(model_name))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load model and get predictions\n",
    "model = tf.keras.models.load_model('lstm_model_1.h5')\n",
    "y_pred = model.predict_classes(X_test_tok_pad, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "pred_classes = [int(n) for n in y_pred]\n",
    "roc_it(y_test, pred_classes, 'LSTM model 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train GRU with same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build and train neural network (~ 70 sec./epoch)\n",
    "checkpoint = callbacks.ModelCheckpoint('gru_model_1.h5', \n",
    "                                       save_best_only=True)\n",
    "\n",
    "embedding_size = 128\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(20000, embedding_size, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train_tok_pad, y_train, epochs=5, batch_size=1024, \n",
    "                    validation_data=(X_val_tok_pad, y_val),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load model and get predictions\n",
    "model = tf.keras.models.load_model('gru_model_1.h5')\n",
    "y_pred = model.predict_classes(X_test_tok_pad, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "pred_classes = [int(n) for n in y_pred]\n",
    "roc_it(y_test, pred_classes, 'GRU model 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train GRU with bigger embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and pad text vectors (~ 1 min.)\n",
    "tokenizer = text.Tokenizer(num_words=200000)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_tok_pad = sequence.pad_sequences(X_train_tok, maxlen=200)\n",
    "\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tok_pad = sequence.pad_sequences(X_val_tok, maxlen=200)\n",
    "\n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_tok_pad = sequence.pad_sequences(X_test_tok, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build and train neural network (~ 2.3 min./epoch)\n",
    "checkpoint = callbacks.ModelCheckpoint('gru_model_2.h5', \n",
    "                                       save_best_only=True)\n",
    "\n",
    "embedding_size = 128\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(200000, embedding_size, input_shape=(200,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(200,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(200,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train_tok_pad, y_train, epochs=5, batch_size=1024, \n",
    "                    validation_data=(X_val_tok_pad, y_val),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load model and get predictions\n",
    "model = tf.keras.models.load_model('gru_model_2.h5')\n",
    "y_pred = model.predict_classes(X_test_tok_pad, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "pred_classes = [int(n) for n in y_pred]\n",
    "roc_it(y_test, pred_classes, 'GRU model 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train GRU with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create sequences of tokens uniform in length for all reviews (~ 1 min.)\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_tok_pad = sequence.pad_sequences(X_train_tok, maxlen=100)\n",
    "\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tok_pad = sequence.pad_sequences(X_val_tok, maxlen=100)\n",
    "\n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_tok_pad = sequence.pad_sequences(X_test_tok, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Try again with more epochs, callbacks\n",
    "checkpoint = callbacks.ModelCheckpoint('gru_model_3.h5',\n",
    "                                       monitor='val_acc', \n",
    "                                       save_best_only=True)\n",
    "# early_stop = callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                      min_delta=0.001, \n",
    "#                                      patience=5) \n",
    "\n",
    "embedding_size = 128\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(200000, embedding_size, \n",
    "                                    input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_tok_pad, y_train, epochs=50, batch_size=2048, \n",
    "                    validation_data=(X_val_tok_pad, y_val),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualize loss and accuracy over training epochs\n",
    "x = [i for i in range(1, 51)]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(x, history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(x, history.history['loss'], label='Train Loss')\n",
    "plt.plot(x, history.history['val_accuracy'], label='Val. Accuracy')\n",
    "plt.plot(x, history.history['val_loss'], label='Val. Loss')\n",
    "plt.title('Model performance over 50 training epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load model and get predictions\n",
    "model = tf.keras.models.load_model('gru_model_2.h5')\n",
    "y_pred = model.predict_classes(X_test_tok_pad, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "pred_classes = [int(n) for n in y_pred]\n",
    "roc_it(y_test, pred_classes, 'GRU model 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('gru_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model using holdout data\n",
    "model.evaluate(X_test_tok_pad, y_test, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Examine classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_classes = model.predict_classes(X_test_tok_pad, batch_size=2048)\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Examine confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a function to plot a color-coded confusion matrix\n",
    "def pretty_confusion(y_true, y_pred, model_name):\n",
    "    '''Display normalized confusion matrix with color scale.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y_true: ground-truth labels\n",
    "    y_pred: predicted labels\n",
    "    model_name: name to print in the plot title\n",
    "    \n",
    "    Dependencies:\n",
    "    numpy aliased as np\n",
    "    sklearn.metrics.confusion_matrix\n",
    "    matplotlib.pyplot aliased as plt\n",
    "    seaborn aliased as sns\n",
    "    '''\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Build the plot\n",
    "    plt.figure(figsize=(16,7))\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(matrix, annot=True, annot_kws={'size':10},\n",
    "                cmap=plt.cm.Greens, linewidths=0.2)\n",
    "    \n",
    "    # Add labels to the plot\n",
    "    class_names = ['Unhelpful', 'Helpful']\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    tick_marks2 = tick_marks + 0.5\n",
    "    plt.xticks(tick_marks, class_names, rotation=25)\n",
    "    plt.yticks(tick_marks2, class_names, rotation=0)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Confusion Matrix for {}'.format(model_name)) \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Define a function to save a confusion matrix visualization    \n",
    "def save_conf_matrix(y_true, y_pred, model_name):\n",
    "    '''Save normalized confusion matrix with color scale as .png file.\n",
    "    \n",
    "    Note that in Jupyter Notebook, the plot will also be displayed\n",
    "    automatically on calling this function. This function saves .png files\n",
    "    with 300 dpi and 0.5-in. padding.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y_true: ground-truth labels\n",
    "    y_pred: predicted labels\n",
    "    model_name: name to print in the plot title\n",
    "    \n",
    "    Dependencies:\n",
    "    sklearn.metrics.confusion_matrix\n",
    "    matplotlib.pyplot aliased as plt\n",
    "    seaborn aliased as sns\n",
    "    '''\n",
    "    fig = pretty_confusion(y_true, y_pred, model_name)\n",
    "    path = '/'\n",
    "    filename = path + '_'.join(model_name.split()) + '_confmatrix.png'\n",
    "    plt.savefig(filename, pad_inches=0.5, dpi=300)\n",
    "\n",
    "# Plot and save the confusion matrix\n",
    "save_conf_matrix(y_test, y_pred_classes, 'Final Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "pred_classes = [int(n) for n in y_pred_classes]\n",
    "roc_it(y_test, pred_classes, 'final model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Inspect sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate a few sample predictions (1 = \"helpful\", 0 = not helpful)\n",
    "sample = X_train_tok_pad[:10]\n",
    "actual = y_train[:10]\n",
    "\n",
    "predictions = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame of predicted v. actual values for samples\n",
    "sample_results = pd.concat([pd.Series([float(p) for p in predictions]), pd.Series(actual).reset_index()], axis=1)\n",
    "sample_results.columns = ['Probability_of_helpfulness', 'Original_index', 'Actual_helpfulness']\n",
    "sample_results.set_index(keys='Original_index', inplace=True)\n",
    "sample_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot predicted v. actual values\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "x = [i for i in range(len(sample_results))]\n",
    "y = [0.5 for i in range(len(sample_results))]\n",
    "colors = ['r', 'r', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g']\n",
    "incorrect = sample_results.Probability_of_helpfulness[:2]\n",
    "correct = sample_results.Probability_of_helpfulness[2:]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(x[:2], incorrect, label='Incorrect prediction', color='r', s=80)\n",
    "plt.scatter(x[2:], correct, label='Correct prediction', color='g', s=80)\n",
    "plt.scatter(x, sample_results.Actual_helpfulness, label='Actual', color='gray', s=80)\n",
    "plt.plot(x, y, linestyle='--', color='k', label='Decision boundary')\n",
    "plt.title('Predicted v. actual helpfulness ratings for sample reviews')\n",
    "plt.xlabel('Indices of sample reviews')\n",
    "plt.xticks(x, sample_results.index)\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.savefig('sample_results.png', dpi=300, padding=0.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
